---
title: "Correlation"
subtitle: "IS381 - Statistics and Probability with R"
author: Jason Bryer, Ph.D.
date: "December 1, 2025"
output:
  xaringan::moon_reader:
    css: ["assets/mtheme_max.css", "assets/fonts_mtheme_max.css"]
    lib_dir: libs
    seal: false
    nature:
      highlightStyle: solarized-light
      highlightLanguage: R
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      navigation:
        scroll: false
    includes:
      in_header: [assets/header.html]
      after_body: [assets/insert-logo.html]
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
# Cartoons from https://github.com/allisonhorst/stats-illustrations
# dplyr based upon https://allisonhorst.shinyapps.io/dplyr-learnr/#section-welcome

source('../config.R')
options(width = 120)

library(VisualStats)
library(ggplot2)
ggplot2::theme_set(ggplot2::theme_minimal())
```

class: center, middle, inverse, title-slide

# `r metadata$title`
## `r metadata$subtitle`
### `r metadata$author`
### `r metadata$date`



---
# Correlation

Correlation is a measure of the relationship between two variables. The correlation can range from -1 indicating a "perfect" negative relationship to 1 indicating a "perfect" positive relationship. A correlation of 0 indicates no relationship.

```{r, echo=FALSE, fig.height=3}
plots <- list()
for(rho in c(-1, -0.8, -0.4, 0, 0.4, 0.8, 1)) {
	df <- mvtnorm::rmvnorm(n = 30,
						   mean = c(0, 0),
						   sigma = matrix(c(1^2, rho * (1 * 1),
						   				 rho * (1 * 1), 1^2), 2, 2)) |>
		as.data.frame()
	
	plots[[length(plots) + 1]] <- ggplot(df, aes(x = V1, y = V2)) + 
		geom_point(size = 0.5) +
		theme_minimal() +
		theme(axis.text = element_blank(), panel.grid = element_blank()) +
		xlab(rho) + ylab('') +
		coord_equal() +
		xlim(c(-3, 3)) + ylim(c(-3, 3))
}
plots$nrow <- 1
do.call(plot_grid, plots)
```

---
# Population Correlation

For a population, the correlation is defined as the ratio of the covariance to the product of the standard deviations, and is typically denoted using the Greek letter rho ($\rho$), is defined as:

$$\rho = \frac{cov(X,Y)}{\sigma_{X} \sigma_{Y}}$$

The standard deviation ($\sigma$) is equal to the square root of the variance ($\sigma = \sqrt{\frac{\Sigma(x_i - \bar{x})^2}{n - 1}}$). What is new here is the covariance. Like variance, we are interested in deviations from the mean except now in two dimensions.

---
# Covariance

The formula for the covariance is:

$$ cov_{xy} = \frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{n - 1} $$

---
# Covariance (Simulated Example)

.pull-left[

```{r}
mean_x <- 20
mean_y <- 40
sd_x <- 2
sd_y <- 3
n <- 30
rho <- 0.8
set.seed(2112)
df <- mvtnorm::rmvnorm(
    n = n,
    mean = c(mean_x, mean_y),
    sigma = matrix(
    	c(sd_x^2, rho * (sd_x * sd_y),
    	  rho * (sd_x * sd_y), sd_y^2), 2, 2)) |>
    as.data.frame() |>
    dplyr::rename(x = V1, y = V2) |>
    dplyr::mutate(
    	x_deviation = x - mean(x),
    	y_deviation = y - mean(y),
    	cross_product = x_deviation * y_deviation)
```

]
.pull-right[
```{r, echo=FALSE, fig.width = 6, fig.height = 6}
regression_vis(df,
				plot_x_mean = FALSE,
				plot_y_mean = FALSE,
				plot_positive_cross_products = FALSE,
				plot_negative_cross_products = FALSE)
```
]

---
# Covariance  ($x - \bar{x}$)

.pull-left[
$$ cov_{xy} = \frac{\Sigma(\mathbf{ x_i - \bar{x} })(y_i - \bar{y})}{n - 1} $$

Consider the point with the largest *x* and *y*-value.

First step in the numerator is to subtract the mean of *x* ($\bar{x}$) from the *x*-value.
]
.pull-right[
```{r, echo=FALSE, fig.width = 6, fig.height = 6}
deviation_to_plot <- 23
regression_vis(df,
				plot_x_mean = TRUE,
				plot_y_mean = TRUE,
				plot_positive_cross_products = FALSE,
				plot_negative_cross_products = FALSE,
				plot_x_deviations = deviation_to_plot,
				plot_y_deviations = NULL) +
	ggplot2::annotate('text', x = 22.5, y = 46.2,
			 label = bquote("x - bar(x)"), parse = TRUE, vjust = 1.5, size = 8)
```
]

---
# Covariance ($y - \bar{y}$)

.pull-left[
$$ cov_{xy} = \frac{\Sigma(x_i - \bar{x})(\mathbf{ y_i - \bar{y} })}{n - 1} $$

Second step in the numerator is to subtract the mean of *y* ($\bar{y}$) from the *y*-value.
]
.pull-right[
```{r, echo=FALSE, fig.width = 6, fig.height = 6}
deviation_to_plot <- 23
regression_vis(df,
				plot_x_mean = TRUE,
				plot_y_mean = TRUE,
				plot_positive_cross_products = FALSE,
				plot_negative_cross_products = FALSE,
				plot_x_deviations = deviation_to_plot,
				plot_y_deviations = deviation_to_plot) +
	ggplot2::annotate('text', x = 25.0, y = 44,
			 label = bquote("y - bar(y)"), parse = TRUE, vjust = 2, size = 8, angle = -90)
```
]

---
# Covariance

.pull-left[
$$ cov_{xy} = \frac{\Sigma(\mathbf{ x_i - \bar{x})(y_i - \bar{y} })}{n - 1} $$

For the first observation, its contribution to the sum is simply the area of the rectangle. We call each of these areas the cross product (i.e. $x_i - \bar{x})(y_i - \bar{y})$).

]
.pull-right[
```{r, echo=FALSE, fig.width = 6, fig.height = 6}
regression_vis(df,
				plot_x_mean = TRUE,
				plot_y_mean = TRUE,
				plot_positive_cross_products = FALSE,
				plot_negative_cross_products = FALSE,
				plot_cross_product = deviation_to_plot,
				plot_x_deviations = deviation_to_plot,
				plot_y_deviations = deviation_to_plot,
				cross_product_alpha = 0.5)
```
]

---
# Covariance (quadrants)

.pull-left[
$$ cov_{xy} = \frac{\Sigma(\mathbf{ x_i - \bar{x})(y_i - \bar{y} })}{n - 1} $$

We can divide the plot into four quadrants split at $\bar{x}$ and $\bar{y}$.

]
.pull-right[
```{r, echo=FALSE, fig.width = 6, fig.height = 6}
regression_vis(df,
				plot_x_mean = TRUE,
				plot_y_mean = TRUE,
				plot_positive_cross_products = FALSE,
				plot_negative_cross_products = FALSE) +
	geom_text(label = '1', x = 22.5, y = 44, color = 'blue', size = 16) +
	geom_text(label = '2', x = 17.5, y = 44, color = 'blue', size = 16) +
	geom_text(label = '3', x = 17.5, y = 36, color = 'blue', size = 16) +
	geom_text(label = '4', x = 22.5, y = 36, color = 'blue', size = 16)
```

]


---
# Covariance (positive cross products)

.pull-left[
$$ cov_{xy} = \frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{n - 1} $$

For observations in quadrant 1, $x - \bar{x}$ is **positive** and $y - \bar{y}$ is **positive** so the cross product is **positive.**

For observations in quadrant 3, $x - \bar{x}$ is **negative** and $y - \bar{y}$ is **negative** so the cross product is **positive.**

Hence, all observations in quadrants 1 and 3 contribute **positively** to the sum of cross products.

]
.pull-right[
```{r, echo=FALSE, fig.width = 6, fig.height = 6}
regression_vis(df,
                plot_x_mean = TRUE,
                plot_y_mean = TRUE,
                plot_positive_cross_products = TRUE,
                plot_negative_cross_products = FALSE)
```
]

---
# Covariance (negative cross products)

.pull-left[
$$ cov_{xy} = \frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{n - 1} $$
For observations in quadrant 2, $x - \bar{x}$ is **negative** and $y - \bar{y}$ is **positive** so the cross product is **negative**

For observations in quadrant 2, $x - \bar{x}$ is **positive** and $y - \bar{y}$ is **negative** so the cross product is **negative**

Hence, all observations in quadrants 2 and 4 contribute **negatively** to the sum of cross products.
]
.pull-right[
```{r, echo=FALSE, fig.width = 6, fig.height = 6}
regression_vis(df,
                plot_x_mean = TRUE,
                plot_y_mean = TRUE,
                plot_positive_cross_products = FALSE,
                plot_negative_cross_products = TRUE)
```
]

---
# Covariance

.pull-left[
$$ cov_{xy} = \frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{n - 1} $$

The covariance is then the ratio of positive cross products to negative cross products.
]

.pull-right[
```{r, echo=FALSE, fig.width = 6, fig.height = 6}
regression_vis(df,
                plot_x_mean = TRUE,
                plot_y_mean = TRUE,
                plot_positive_cross_products = TRUE,
                plot_negative_cross_products = TRUE)
```
]


---
# Covariance

.pull-left[
$$ cov_{xy} = \frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{n - 1} $$

The covariance is then the ratio of positive cross products to negative cross products.

Which can be more easily seen by looking at a histogram of cross products.
]
.pull-right[
```{r, echo=FALSE, fig.width = 6, fig.height = 6}
ggplot(df, aes(x = cross_product, fill = cross_product > 0)) +
    geom_histogram(bins = 15, alpha = 0.75) +
    scale_fill_manual('Cross product > 0', values = c('lightblue', 'darkred')) +
    xlab('Cross Product') + 
    theme_vs()
```
]


---
# Sample Correlation

Putting it all together we get...

$${ r }_{ xy }=\frac { \frac { \sum _{ i=1 }^{ n }{ \left( { X }_{ i }-\overline { X }  \right) \left( { Y }_{ i }-\overline { Y }  \right)  }  }{ n-1 }  }{ { s }_{ x }{ s }_{ y } }$$

--

Interestingly, if we have standardized scores (i.e. *z*-scores where mean = 0 and standard deviation = 1), we can simplify the correlation calculation...

$${ r }_{ xy }=\frac { \frac { \sum _{ i=1 }^{ n }{ \left( { X }_{ i }-\overline { X }  \right) \left( { Y }_{ i }-\overline { Y }  \right)  }  }{ n-1 }  }{ { s }_{ x }{ s }_{ y } }=\frac{\frac{\sum_{ i=1 }^{ n }{ \left( { X }_{ i }-0  \right) \left( { Y }_{ i }-0  \right)  }  }{ n-1 }  }{ 1 \times 1 } = \frac{\sum_{i=1}^{n}{X_i Y_i}}{n - 1}$$


--
Try the following shiny application with various correlations.

```{r, eval=FALSE}
VisualStats::regression_shiny()
```



---
# Example: SAT Scores

What is the correlation between SAT math and verbal scores?

To begin, we read in the CSV file and convert the `Verbal` and `Math` columns to integers. The data file uses `.` (i.e. a period) to denote missing values. The `as.integer` function will automatically convert those to `NA` (the indicator for a missing value in R). Finally, we use the `complete.cases` eliminate any rows with any missing values.

```{r, warning=FALSE}
sat <- read.csv('SAT_scores.csv', stringsAsFactors=FALSE)
names(sat) <- c('Verbal','Math','Sex')
sat$Verbal <- as.integer(sat$Verbal)
sat$Math <- as.integer(sat$Math)
sat <- sat[complete.cases(sat),]
```

---
# Scatter Plot

The first step is to draw a scatter plot. We see that the relationship appears to be fairly linear.

```{r scatterplot, tidy=FALSE, echo=FALSE}
ggplot(sat, aes(x=Verbal, y=Math)) + geom_point(color='black')
```

---
# Descriptive Statistics

.pull-left[

Next, we will calculate the means and standard deviations.

```{r descriptives}
( verbalMean <- mean(sat$Verbal) )
( mathMean <- mean(sat$Math) )
```

]
.pull-right[

```{r descriptives2}
( verbalSD <- sd(sat$Verbal) )
( mathSD <- sd(sat$Math) )
( n <- nrow(sat) )
```

]

---
# Covariance

The population correlation, rho,  is defined as ${ \rho  }_{ xy }=\frac { { \sigma  }_{ xy } }{ { \sigma  }_{ x }{ \sigma  }_{ y } }$ where the numerator is the *covariance* of *x* and *y* and the denominator is the product of the two standard deviations.

--

The sample correlation is calculated as ${ r }_{ xy }=\frac { { Cov }_{ xy } }{ { s }_{ x }{ s }_{ y } }$

--

The covariates is calculated as ${ Cov }_{ xy }=\frac { \sum _{ i=1 }^{ n }{ \left( { X }_{ i }-\overline { X }  \right) \left( { Y }_{ i }-\overline { Y }  \right)  }  }{ n-1 }$

--

```{r}
(cov.xy <- sum( (sat$Verbal - verbalMean) * (sat$Math - mathMean) ) / (n - 1))
```

Or we can use the built-in `cov` function.

```{r}
cov(sat$Verbal, sat$Math)
```

---
# Covariance (cont.) 

$${ r }_{ xy }=\frac { \frac { \sum _{ i=1 }^{ n }{ \left( { X }_{ i }-\overline { X }  \right) \left( { Y }_{ i }-\overline { Y }  \right)  }  }{ n-1 }  }{ { s }_{ x }{ s }_{ y } }$$

```{r}
cov.xy / (verbalSD * mathSD)
```

Or we can use the built-in `cor` function.

```{r}
cor(sat$Verbal, sat$Math)
```

---
# Using *z*-Scores

Calculate z-scores (standard scores) for the verbal and math scores.

$$ z=\frac { y-\overline { y }  }{ s } $$

```{r zscores}
sat$Verbal.z <- (sat$Verbal - verbalMean) / verbalSD
sat$Math.z <- (sat$Math - mathMean) / mathSD
head(sat)
```


---
# Correlation

Calculate the correlation manually using the z-score formula:

$$r=\frac { \sum { { z }_{ x }{ z }_{ y } }  }{ n-1 }$$

```{r correlation}
r <- sum( sat$Verbal.z * sat$Math.z ) / ( n - 1 )
r
```

--

.pull-left[

We can see that this matches the correlation using the unstandardized values.

```{r}
cor(sat$Verbal, sat$Math)
```

]

--

.pull-right[

And to show that the units don't matter, calculate the correlation with the z-scores.
```{r}
cor(sat$Verbal.z, sat$Math.z)
```

]

---
class: font90
# Is the correlation different than zero?

.pull-left[
Just because we have a non-zero correlation does not necessarily mean the correlation is *statistically* different from zero. We can conduct a null hypothesis test where:

* $H_0$: The correlation is zero.
* $H_A$: The correlation is not equal to zero.

The `cor.test` function will perform that null hypothesis test providing both the *p*-value and confidence interval.

]
.pull-right[
```{r}
cor.test(sat$Verbal.z, sat$Math.z)
```
]

--

The following Shiny application will allow for estimating the sampling distribution for varying correlations between -1 and 1. Be sure to note the relationship of sample size to the confidence interval, especially when the population correlation is zero.

```{r, eval=FALSE}
shiny::runApp(paste0(find.package('VisualStats'), '/shiny/simulated_correlation'))
```


---
class: left, font140
# One Minute Paper

.pull-left[
1. What was the most important thing you learned during this class?
2. What important question remains unanswered for you?
]
.pull-right[
```{r, echo=FALSE, fig.width=5, fig.height=5}
qrcode::qr_code(one_minute_paper) |> plot(col = c('#FAFAFA', 'black'))
```
]

`r one_minute_paper`
